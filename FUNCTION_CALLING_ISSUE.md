# Function Calling Issue - Root Cause Analysis

## üîç Issue Summary

The `quick_code_review` workflow terminates without completing because the CodeAnalyzer agent attempts to call `read_file()` but the function never executes.

---

## ‚úÖ What's Working

### Diagnostic Results (All Passing!)

```
‚úÖ Agents created: 8
‚úÖ Functions registered: 19
‚úÖ read_file function available
‚úÖ Executor has function_map with 19 functions
‚úÖ MCP servers running (GitHub, Filesystem, Memory, Slack)
‚úÖ Tools properly added to llm_config
```

**Everything is configured correctly!**

---

## ‚ùå Root Cause

### The Model's Function Calling Format

The model `openai/gpt-oss-120b:free` is generating function calls in an **incorrect format**:

**What the model generates:**
```json
{
  "action": "read_file",
  "arguments": {"file_path": "./main.py"}
}
```

**What AutoGen expects (OpenAI format):**
```json
{
  "name": "read_file",
  "arguments": {"file_path": "./main.py"}
}
```

The model uses `"action"` instead of `"name"`, which AutoGen doesn't recognize as a valid function call.

---

## üîß Solutions

### Option 1: Switch to a Better Model (Recommended)

Use a model that properly supports OpenAI function calling format:

**Edit:** `config/autogen_agents.yaml`

```yaml
llm_configs:
  code_analysis_config:
    # Old (not working well):
    # model: "openai/gpt-oss-120b:free"

    # New (better function calling support):
    model: "mistralai/mixtral-8x7b-instruct"  # Good function calling
    # or
    model: "meta-llama/llama-3.1-70b-instruct"  # Excellent function calling
    # or
    model: "anthropic/claude-3-haiku"  # Best function calling (paid)

    api_type: "openai"
    base_url: "https://openrouter.ai/api/v1"
    api_key: "${OPENROUTER_API_KEY}"
    temperature: 0.3
    max_tokens: 4096
```

**Benefits:**
- ‚úÖ Proper OpenAI function calling format
- ‚úÖ Better reliability
- ‚úÖ More accurate function parameter extraction

---

### Option 2: Add Response Parsing (Complex)

Modify the agent to parse the model's custom format and convert it to OpenAI format. This requires changes to the AutoGen conversation flow.

**Not recommended** - switching models is much simpler.

---

### Option 3: Use a Local Function Calling Wrapper

Create a wrapper that intercepts the model's response and reformats it.

**Implementation complexity:** High
**Maintenance:** Difficult
**Recommendation:** Not worth it - use Option 1

---

## üìä Model Comparison

| Model | Function Calling | Free | Speed | Quality |
|-------|-----------------|------|-------|---------|
| `openai/gpt-oss-120b:free` | ‚ö†Ô∏è Non-standard | ‚úÖ Yes | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| `mistralai/mixtral-8x7b-instruct` | ‚úÖ OpenAI format | üí∞ Paid | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `meta-llama/llama-3.1-70b-instruct` | ‚úÖ OpenAI format | üí∞ Paid | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `anthropic/claude-3-haiku` | ‚úÖ Perfect | üí∞ Paid | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üéØ Recommended Action

### Quick Fix (Immediate)

1. **Get an OpenRouter API key** (if you don't have credits)
   - Visit: https://openrouter.ai/
   - Add payment method
   - Get $5-10 credits (goes a long way)

2. **Update the model** in `config/autogen_agents.yaml`:
   ```yaml
   model: "meta-llama/llama-3.1-70b-instruct"
   ```

3. **Restart the application**
   ```bash
   python main.py
   ```

4. **Test again**
   ```bash
   >>> run quick_code_review code_path=./main.py focus_areas="security"
   ```

---

## üîç Why This Happens

### Free vs Paid Models

**Free models** like `gpt-oss-120b:free`:
- Limited function calling training
- May use non-standard formats
- Less reliable for complex tool use

**Paid models** like `llama-3.1-70b-instruct`:
- Extensively trained on OpenAI function calling
- Follow standard formats
- Reliable tool execution

---

## ‚úÖ Alternative: Keep Free Model, Reduce Workflow Complexity

If you must use the free model, simplify the workflow:

### 1. Increase max_turns to 15-20
Already done ‚úÖ (set to 10, can increase more)

### 2. Simplify the agent's instructions
Remove complex function calling requirements and use simpler prompts

### 3. Use direct file reading in the initial message
Instead of asking the agent to read the file, read it yourself and include it in the prompt:

```python
# Read file first
with open("./main.py") as f:
    code_content = f.read()

# Then pass to workflow
run quick_code_review code_content="{code_content}" focus_areas="security"
```

---

## üìù Summary

**Problem:** Free model uses non-standard function calling format
**Impact:** Functions don't execute, workflows terminate early
**Solution:** Use a model with proper OpenAI function calling support

**Setup Status:**
- ‚úÖ All MCP servers working
- ‚úÖ All functions registered correctly
- ‚úÖ Agents configured properly
- ‚ùå Model doesn't generate proper function calls

**Next Step:** Switch to a better model or work around the limitation

---

**Estimated Cost with Paid Model:**
- Llama 3.1 70B: ~$0.80 per 1M tokens
- Typical code review: ~5,000 tokens = **$0.004 (less than 1 cent)**
- 100 code reviews: **$0.40**

Very affordable for production use!
