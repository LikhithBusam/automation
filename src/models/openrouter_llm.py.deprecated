"""
OpenRouter LLM Wrapper for CrewAI
Provides integration with OpenRouter API for model access
"""

import os
import logging
import requests
from typing import Any, Dict, List, Optional
from langchain_core.language_models import BaseLLM
from langchain_core.callbacks import CallbackManagerForLLMRun
from langchain_core.outputs import LLMResult, Generation
from pydantic import Field


class OpenRouterLLM(BaseLLM):
    """Custom LLM wrapper for OpenRouter API"""

    model: str = Field(default="arcee-ai/trinity-mini:free")
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=2048)
    api_key: str = Field(...)
    api_base: str = Field(default="https://openrouter.ai/api/v1")

    @property
    def _llm_type(self) -> str:
        return "openrouter"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call OpenRouter API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/yourusername/automaton",
            "X-Title": "Intelligent Development Assistant",
        }

        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
        }

        if stop:
            data["stop"] = stop

        response = requests.post(
            f"{self.api_base}/chat/completions",
            headers=headers,
            json=data,
            timeout=60
        )
        response.raise_for_status()

        result = response.json()
        return result["choices"][0]["message"]["content"]

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> LLMResult:
        """Generate responses for multiple prompts"""
        generations = []
        for prompt in prompts:
            text = self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
            generations.append([Generation(text=text)])

        return LLMResult(generations=generations)


def create_openrouter_llm(
    model: str = "arcee-ai/trinity-mini:free",
    temperature: float = 0.7,
    max_tokens: int = 2048,
    api_key: Optional[str] = None,
    **kwargs
) -> BaseLLM:
    """
    Create an OpenRouter LLM instance compatible with CrewAI

    Args:
        model: OpenRouter model name (default: arcee-ai/trinity-mini:free)
        temperature: Temperature for generation
        max_tokens: Maximum tokens to generate
        api_key: OpenRouter API key (defaults to OPENROUTER_API_KEY env var)
        **kwargs: Additional arguments to pass to OpenRouterLLM

    Returns:
        BaseLLM instance configured for OpenRouter
    """
    logger = logging.getLogger("models.openrouter")

    # Get API key from parameter or environment
    openrouter_api_key = api_key or os.getenv("OPENROUTER_API_KEY")

    if not openrouter_api_key:
        raise ValueError(
            "OpenRouter API key not found. Please set OPENROUTER_API_KEY environment variable "
            "or pass api_key parameter."
        )

    logger.info(f"Creating OpenRouter LLM with model: {model}")

    # Create OpenRouterLLM instance
    llm = OpenRouterLLM(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        api_key=openrouter_api_key,
        **kwargs
    )

    logger.info("OpenRouter LLM created successfully")
    return llm


def get_default_openrouter_config() -> Dict[str, Any]:
    """
    Get default OpenRouter configuration for different agent types

    Returns:
        Dictionary mapping agent types to their recommended configurations
    """
    return {
        "code_analyzer": {
            "model": "arcee-ai/trinity-mini:free",
            "temperature": 0.3,
            "max_tokens": 4096,
        },
        "documentation": {
            "model": "arcee-ai/trinity-mini:free",
            "temperature": 0.7,
            "max_tokens": 2048,
        },
        "deployment": {
            "model": "arcee-ai/trinity-mini:free",
            "temperature": 0.2,
            "max_tokens": 2048,
        },
        "research": {
            "model": "arcee-ai/trinity-mini:free",
            "temperature": 0.5,
            "max_tokens": 4096,
        },
        "project_manager": {
            "model": "arcee-ai/trinity-mini:free",
            "temperature": 0.7,
            "max_tokens": 2048,
        },
    }
